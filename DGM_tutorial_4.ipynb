{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPh5LZGoP8gqUPiL7bkSyc6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/paulsubarna/BLIP/blob/main/DGM_tutorial_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tutorial 4: Variational AutoEncoder (Cont.)\n",
        "\n",
        "We will continue with our implementation of VAEs. The previous tutorial gave us an overview of the steps to model a vanilla VAE. In this tutorial, we will try to delve a bit deeper into the architectures, exploring complex architectures to build our encoder and decoder modules and also try to answer some analytical questions !!\n",
        "\n",
        "References:\n",
        "https://docs.pytorch.org/docs/stable/generated/torch.nn.Conv2d.html"
      ],
      "metadata": {
        "id": "R7YPmtGTtB08"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "l8Ei1Ljqd5C2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convolutional VAEs\n",
        "\n",
        "TO-DO:\n",
        "1. Recreate the VAE model with convolutional layers in the Encoder and the Decoder\n"
      ],
      "metadata": {
        "id": "IHREglU0uKzQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class VAE(nn.Module):\n",
        "\n",
        "  def __init__(self, in_channels, out_channels, latent_dim):\n",
        "    super(VAE, self).__init__()\n",
        "\n",
        "    self.latent_dim = latent_dim\n",
        "    self.in_channels = in_channels\n",
        "    self.out_channels = out_channels\n",
        "\n",
        "    modules = []\n",
        "\n",
        "    hidden_dims = [32, 64, 128, .... ]\n",
        "\n",
        "\n",
        "    ### Construct your Encoder\n",
        "\n",
        "    ### Construct your decoder\n",
        "\n",
        "\n",
        "    # Upsample the decoder output to match 28x28 size\n",
        "\n",
        "\n",
        "\n",
        "  def encode(self, x):\n",
        "    latent_ = self.encoder(x)\n",
        "    latent_ = torch.flatten(latent_, start_dim=1)\n",
        "    mu = self.mu(latent_)\n",
        "    log_var = self.log_var(latent_)\n",
        "    return mu, log_var\n",
        "\n",
        "\n",
        "  def decode(self, z):\n",
        "    result = self.decoder_input(z)\n",
        "    result = result.view(-1, 512, 2, 2)\n",
        "    result = self.decoder(result)\n",
        "    x_ = self.final_layer(result)\n",
        "    return x_\n",
        "\n",
        "  def reparameterize(self, mu, log_var):\n",
        "    std = torch.exp(0.5 * log_var)\n",
        "    eps = torch.randn_like(std)\n",
        "    z = mu + eps * std\n",
        "    return z\n",
        "\n",
        "  def sample(self, num_samples):\n",
        "    z = torch.randn(num_samples, self.latent_dim)\n",
        "    samples = self.decode(z)\n",
        "    return samples\n",
        "\n",
        "  def generate(self, x):\n",
        "    return self.forward(x)[0]\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "\n",
        "    return x_reconstructed, mu, log_var\n",
        "\n",
        "  def loss_function(self, x, recons, mu, log_var):\n",
        "    reconstruction_loss =\n",
        "    kl_divergence =\n",
        "    return reconstruction_loss + kl_divergence\n"
      ],
      "metadata": {
        "id": "JistclFFeBVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Loading: Mnist\n",
        "\n",
        "1. In this work, you will load Mnist dataset from torchvision.\n",
        "2. Follow the same steps from the previous tutorials to load and preprocess the dataset\n",
        "\n",
        "Reference:\n",
        "https://docs.pytorch.org/vision/stable/datasets.html"
      ],
      "metadata": {
        "id": "vLJ5CMQ7h-L-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "np1JsZFPsA-G",
        "outputId": "cc09dc51-65a2-47ea-d45b-869969b4397c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:01<00:00, 4.99MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 131kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:01<00:00, 1.23MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 14.9MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "def train(model, train_loader, optimizer, epoch, device):\n",
        "  model.train()\n",
        "  running_loss = 0.0\n",
        "  for e in range(epoch):\n",
        "    for batch_idx, batch in tqdm(enumerate(train_loader)):\n",
        "      data, _ = batch\n",
        "      data = data.to(device)\n",
        "      (out, mu, log_var) = model(data)\n",
        "      loss = model.loss_function(data,out, mu, log_var)\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      running_loss += loss.item()\n",
        "\n",
        "    print(f'loss after {e} epochs: {running_loss/(batch_idx+1)}')\n",
        "    running_loss = 0.0\n",
        "  return model\n",
        "\n",
        "\n",
        "def test(model, test_loader, device):\n",
        "  model.eval()\n",
        "  test_loss = 0\n",
        "  with torch.no_grad():\n",
        "    for data, _ in test_loader:\n",
        "      data = data.to(device)\n",
        "      (out, mu, log_var) = model(data)\n",
        "      test_loss += model.loss_function\n",
        "      test_loss /= len(test_loader.dataset)\n",
        "  print(f'Test loss: {test_loss}')\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MDrZtoOLspoX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define your Hyperparameters\n"
      ],
      "metadata": {
        "id": "AZ6rF9FuiZPt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr =\n",
        "epoch=\n",
        "latent_dim =\n",
        "in_channels =\n",
        "out_channels =\n",
        "device = torch.device('cuda')\n",
        "model = VAE(in_channels=in_channels, out_channels=out_channels, latent_dim=20)\n",
        "model.to('cuda')\n",
        "train(model, train_loader, optim.Adam(model.parameters(), lr=lr), epoch, device)"
      ],
      "metadata": {
        "id": "kUR3g6-yvBDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizaion\n",
        "\n",
        "1. Compare the Real and Generated Images from your VAE model\n"
      ],
      "metadata": {
        "id": "NrYhxXa2i1bm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "data = next(iter(test_loader))[0]#.to(device)\n",
        "recons = model.generate(data.to(device))\n",
        "\n",
        "fig, axs = plt.subplots(4, 4)\n",
        "for i, ax in enumerate(axs.flatten()):\n",
        "  ax.imshow(data[i].cpu().detach().numpy().squeeze(), cmap='gray')\n",
        "plt.show()\n",
        "\n",
        "fig1, axs1 = plt.subplots(4, 4)\n",
        "\n",
        "for i, ax in enumerate(axs1.flatten()):\n",
        "  ax.imshow(recons[i].cpu().detach().numpy().squeeze(), cmap='gray')"
      ],
      "metadata": {
        "id": "1NnDIDutvjN_",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#data = next(iter(test_loader))#.to(device)\n",
        "plt.imshow(data[1][0])#.shape\n",
        "#data[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "hRghd5TV6z43",
        "outputId": "b626c1d5-b46a-43b0-f8e8-42c2e2b4feee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7ae7aad5c7d0>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Aggregate Posterior\n",
        "\n",
        "1. Now that you have a trained VAE model, iterate through the entire dataset, testset or validation set, to compute your aggregate posterior.\n",
        "2. If the dimension is greater than 2, reduce the dimension using PCA or TSNE and visualize it in a scatter plot.\n",
        "3. Now that you have produced the scatter plot, pick random samples from posterior, typically one closer to mean and one farther away.\n",
        "4. Compare the generated image from these two distinct randomly sample point using your trained decoder and comment your findings !!\n"
      ],
      "metadata": {
        "id": "PPulYB8Yi_tw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## TO-DO"
      ],
      "metadata": {
        "id": "4H1L0CmGkIcO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yqJWDvGH7GXV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}